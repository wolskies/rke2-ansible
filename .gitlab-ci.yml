# SPDX-License-Identifier: GPL-3.0-or-later
# GitLab CI/CD Pipeline for RKE2 Ansible Collection
---
stages:
  - lint
  - unit-test
  - build
  - security-scan
  - deploy-test

variables:
  ANSIBLE_COLLECTIONS_PATH: "${CI_PROJECT_DIR}"
  ANSIBLE_HOST_KEY_CHECKING: "False"
  ANSIBLE_STDOUT_CALLBACK: "yaml"
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  UV_CACHE_DIR: "$CI_PROJECT_DIR/.cache/uv"
  # Self-hosted optimization
  FF_USE_FASTZIP: "true"
  ARTIFACT_COMPRESSION_LEVEL: "fast"
  CACHE_COMPRESSION_LEVEL: "fast"

# Template for common job configuration - optimized for self-hosted
.ansible_job_template: &ansible_job
  image: python:3.13-slim-bookworm
  before_script:
    # Use uv for faster package installation
    - apt update && apt install curl -y
    - curl -LsSf https://astral.sh/uv/install.sh | sh
    - source $HOME/.local/bin/env
    - uv venv .venv
    - source .venv/bin/activate
    - uv pip install -r test-requirements.txt
    - |
      # Install collections with retry logic and fallback
      for i in {1..3}; do
        if ansible-galaxy collection install -r requirements.yml --force; then
          echo "Collections installed successfully"
          break
        else
          echo "Attempt $i failed, retrying in 10 seconds..."
          sleep 10
          if [ $i -eq 3 ]; then
            echo "All attempts failed, trying individual collection install"
            ansible-galaxy collection install community.docker --force || \
              echo "WARNING: community.docker install failed"
            ansible-galaxy collection install kubernetes.core --force || \
              echo "WARNING: kubernetes.core install failed"
            ansible-galaxy collection install community.general --force || \
              echo "WARNING: community.general install failed"
          fi
        fi
      done
  cache:
    key:
      files:
        - test-requirements.txt
        - requirements.yml
    paths:
      - .cache/uv/
      - .venv/
      - ~/.ansible/collections/
  tags:
    - docker # Use your self-hosted runners with docker tag

# Linting Stage
lint:ansible-lint:
  <<: *ansible_job
  stage: lint
  script:
    - ansible-lint --version
    # Clean up any existing collections to avoid duplicates
    - rm -rf ansible_collections .ansible 2>/dev/null || true
    # Create ansible-lint config to skip community modules
    - |
      cat > .ansible-lint << 'EOF'
      exclude_paths:
        - .ansible/
        - ansible_collections/
        - .venv/
        - .cache/
      skip_list:
        - var-naming[no-role-prefix]
      warn_list:
        - jinja[spacing]
        - name[template]
      EOF
    - ansible-lint roles/ playbooks/ --format json | tee ansible-lint-report.json
  artifacts:
    reports:
      codequality: ansible-lint-report.json
    when: always
    expire_in: 1 week
  allow_failure: false

lint:yaml:
  <<: *ansible_job
  stage: lint
  script:
    - yamllint --version
    - yamllint -c .yamllint.yml .
  allow_failure: false

# Unit Testing Stage
test:syntax:
  <<: *ansible_job
  stage: unit-test
  before_script:
    # Minimal setup for syntax check only - skip external collections
    - apt update && apt install curl -y
    - curl -LsSf https://astral.sh/uv/install.sh | sh
    - source $HOME/.local/bin/env
    - uv venv .venv
    - source .venv/bin/activate
    - uv pip install ansible-core yamllint
  script:
    - |
      for role in roles/*/; do
        echo "Testing syntax for role: $(basename $role)"
        ansible-playbook --syntax-check tests/syntax/test-$(basename $role).yml || exit 1
      done
  parallel:
    matrix:
      - ROLE:
          [
            deploy_rke2,
            helm_install,
            longhorn_install,
            mysql_operator,
            rancher_install,
            rook_install,
            teardown,
          ]

test:molecule:
  <<: *ansible_job
  stage: unit-test
  services:
    - name: docker:24.0.7-dind
      alias: docker
      command: ["--tls=false", "--host=tcp://0.0.0.0:2375"]
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ""
  before_script:
    - apt update && apt install curl docker.io -y
    - curl -LsSf https://astral.sh/uv/install.sh | sh
    - source $HOME/.local/bin/env
    - uv venv .venv
    - source .venv/bin/activate
    - uv pip install -r test-requirements.txt
    - ansible-galaxy collection install community.docker kubernetes.core community.general --force
  script:
    - echo "Waiting for Docker daemon to start..."
    - timeout 120 sh -c "until docker info >/dev/null 2>&1; do echo 'Connecting to Docker...'; sleep 3; done"
    - docker info
    - |
      roles="deploy_rke2 helm_install longhorn_install"
      roles="$roles mysql_operator rancher_install rook_install teardown"
      for role in $roles; do
        echo "Testing role: $role"
        cd roles/$role
        molecule test --scenario-name default || true
        cd ../..
      done
  artifacts:
    paths:
      - roles/*/molecule/default/
    when: always
    expire_in: 1 week
  allow_failure: true

# Integration tests skipped - Docker container tests provide limited value
# TODO: Implement Kind-based testing for realistic K8s deployment testing

# Build Stage
build:collection:
  <<: *ansible_job
  stage: build
  script:
    - ansible-galaxy collection build --force
    - ansible-galaxy collection install wolskinet-rke2_ansible-*.tar.gz --force
  artifacts:
    paths:
      - "*.tar.gz"
    expire_in: 1 month
  only:
    - main
    - tags

# Security Scanning Stage
security:ansible-lint:
  <<: *ansible_job
  stage: security-scan
  script:
    - ansible-lint --version
    - ansible-lint roles/ playbooks/ --format json > security-report.json || true
  artifacts:
    paths:
      - security-report.json
    when: always
    expire_in: 1 week
  allow_failure: true

security:secrets:
  image: trufflesecurity/trufflehog:latest
  stage: security-scan
  script:
    - trufflehog filesystem --directory=. --json > secrets-scan.json
  artifacts:
    paths:
      - secrets-scan.json
    when: always
    expire_in: 1 week
  allow_failure: true

# Deployment Test Stage - Kind-based testing
deploy-test:kind-cluster:
  image: docker:24.0.7-dind
  stage: deploy-test
  services:
    - name: docker:24.0.7-dind
      alias: docker
      command: ["--tls=false", "--host=tcp://0.0.0.0:2375"]
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ""
  before_script:
    # Install dependencies
    - apk add --no-cache curl bash openssh-client python3 py3-pip

    # Install Kind
    - curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64
    - chmod +x ./kind && mv ./kind /usr/local/bin/

    # Install kubectl
    - curl -LO "https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl"
    - chmod +x kubectl && mv kubectl /usr/local/bin/

    # Install Ansible
    - pip3 install ansible-core ansible

    # Wait for Docker daemon
    - timeout 120 sh -c "until docker info >/dev/null 2>&1; do echo 'Connecting to Docker...'; sleep 3; done"

  script:
    # Create Kind cluster with Ubuntu compatibility
    - ./tests/kind/setup-ubuntu-kind.sh

    # Test Ansible connectivity
    - ANSIBLE_CONFIG=tests/kind/ansible.cfg ansible -i tests/kind/simple-inventory.ini all -m ping

    # Run Kind test playbook
    - ANSIBLE_CONFIG=tests/kind/ansible.cfg ansible-playbook \
        -i tests/kind/simple-inventory.ini tests/kind/test-playbook.yml -v

    # Verify cluster state
    - kubectl get nodes
    - kubectl get pods -A

  after_script:
    # Cleanup
    - kind delete cluster --name rke2-test || true

  artifacts:
    paths:
      - tests/kind/*.log
    when: always
    expire_in: 1 week

  # Run manually or on specific branches
  when: manual
  only:
    - main
    - develop
    - /^feature\/kind-testing$/
